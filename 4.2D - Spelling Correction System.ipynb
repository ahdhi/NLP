{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8f3d28-ea1c-4c35-9aa6-d8d4654e9e01",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will embark on the exciting task of constructing a spell checker utilizing the noisy channel model. Your mission involves the implementation of a segment dedicated to the noisy-channel model for spelling correction, complemented by the integration of diverse language models. During the testing phase, you will encounter sentences intentionally infused with a single typing error. The objective is to meticulously identify and rectify the error by selecting the correction that attains the highest likelihood under the noisy-channel model. Additionally, your language model will serve as the crucial prior in this correction process. The effectiveness of your spell checker will be assessed based on accuracy, calculated as the ratio of valid corrections to the total number of test sentences. Prepare to delve into the intricacies of language modeling for spell-checking.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dbb81-a456-4226-a36f-54b20494a723",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "In this assignment, you will be utilizing three essential class structures provided to you: `LexicalEntry`, `Sentence`, and `Corpus`. \n",
    "Before delving into the core tasks of the assignment, it is imperative to familiarize yourself with these foundational structures. Each class serves a distinct purpose in facilitating your understanding of lexical entries, sentence structures, and corpus organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd75d123-1fa4-4469-aba7-2e99fa491a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LexicalEntry, Sentence, Corpus, SpellingResult, group_n_words\n",
    "import collections\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f57137-981d-4fb6-9241-255b78bfbe92",
   "metadata": {},
   "source": [
    "## LexicalEntry\n",
    "\n",
    "The `LexicalEntry` class represents a unit of information about a word and its potential error within the context of a spelling correction system. Here's a summary of its key functionalities:\n",
    "\n",
    "- **Initialization:** The class is initialized with a correct `word` and an optional `error` word, which defaults to an empty string if not provided.\n",
    "- **Fixing Error:** The `fixError` method creates a new `LexicalEntry` object with the same correct word but an empty error attribute, essentially fixing the error.\n",
    "- **Error Checking:** The `hasError` method checks if the `LexicalEntry` object has an error, returning **True** if an error is present and **False** otherwise.\n",
    "- **Validity Testing:** The `isValidTest` method determines if the error in the `LexicalEntry` is within an edit distance of **one** and contains no numerics or punctuation. It returns **True** if the conditions are met, and **False** otherwise.\n",
    "- **String Representation:** The `__str__` method creates a string representation of the `LexicalEntry` object in the format **word (error)** if an error is present, and simply **word** if there is no error.\n",
    "\n",
    "This class provides a comprehensive set of methods to manage and analyze lexical entries, making it a crucial component in the broader spell-checking system. It encompasses functionality for error correction, error checking, validity testing, and string representation of lexical entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eab299-fdfd-43bf-b038-f18bd7a7058d",
   "metadata": {},
   "source": [
    "### Usage examples\n",
    "\n",
    "Let's instantiate a `LexicalEntry` object with the correct word **love** and an associated error word **lov**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452056ef-1faf-48f2-9bde-e14f7d16b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_entry = LexicalEntry(\"love\", \"lov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8089e60-0bac-4191-a929-86ba002476d9",
   "metadata": {},
   "source": [
    "Let's check if there is an error in that `LexicalEntry` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7faf42d-2ee5-4d17-a474-91fe0fa09d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_entry.hasError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473888d-f40f-4a46-89ee-6d867f2b2d1c",
   "metadata": {},
   "source": [
    "Let's check if the error is withing Edit distance of 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c4eba4-d373-4faa-af8b-6e6b449e8c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_entry.isValidTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473d98a-e402-470b-9409-a7d8178a7bc0",
   "metadata": {},
   "source": [
    "Let's now print it using the `__str__` method to show the error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3179f-4e30-43f5-9c8d-dd91971f2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lexical_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f90bf-3634-4a31-be23-3bb07bab0e5c",
   "metadata": {},
   "source": [
    "Finally, let's fix the error and print it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4efd5-fdda-4071-bb6f-424c2e320bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_entry_fixed = lexical_entry.fixError()\n",
    "print(lexical_entry_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f73fa4-9e87-4827-bae8-931fc25e20a1",
   "metadata": {},
   "source": [
    "## Sentence\n",
    "\n",
    "The `Sentence` class is designed to manage and manipulate a sequence of `LexicalEntry` instances, each representing a word with potential errors in a sentence. Here is a summary of its key methods:\n",
    "\n",
    "- **Initialization:** The class is initialized with a list of `LexicalEntries` representing the words in a sentence. The default is an empty list if not provided.\n",
    "- **Error and Correction Retrieval:** Methods such as `getErrorSentence` and `getCorrectSentence` return lists of strings containing the sentence with errors or corrections, respectively.\n",
    "- **Correction Verification:** The `isCorrection` method checks if a given list of strings is a correction of the sentence.\n",
    "- **Error Index Detection:** The `getErrorIndex` method returns the index of the first error in the sentence or -1 if there is no error.\n",
    "- **Index-based Access and Modification:** Methods like `get` retrieve the `LexicalEntry` at a specified index, while `update` modifies the entry at a given index.\n",
    "- **Sentence Cleaning:** The `cleanSentence` method creates a new sentence with all `LexicalEntry` instances having errors removed.\n",
    "- **Empty Check:** The `isEmpty` method checks if the sentence is empty.\n",
    "- **Appending and Length Calculation:** The `append` method adds a `LexicalEntry` to the sentence, and the `__len__` method returns the length of the sentence.\n",
    "- **String Representation:** The `__str__` method generates a string representation of the `Sentence` object, combining the string representations of individual `LexicalEntry` instances in the sentence.\n",
    "\n",
    "\n",
    "Overall, the `Sentence` class provides a robust set of methods for managing and analyzing sentences with potential spelling errors, making it a valuable component in the context of spelling correction systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a95b8-c34c-4f8f-b9ca-4fb5157d9aac",
   "metadata": {},
   "source": [
    "### Usage examples\n",
    "\n",
    "Let's instantiate a `Sentence` object with the sentence **\"i study at Deakin University\"** with a correct word **study** and an associated error word **stud**. . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7978924-c5ed-4a1b-b146-d1d2da1b9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [LexicalEntry(\"i\"), LexicalEntry(\"study\", \"stud\"), LexicalEntry(\"at\"), LexicalEntry(\"Deakin\"), LexicalEntry(\"University\")]\n",
    "my_sentence = Sentence(lst)\n",
    "# Print the sentence\n",
    "print(my_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b388f26-ac35-47eb-af27-7ab442718e07",
   "metadata": {},
   "source": [
    "Let's first print the error sentence and the correct sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa7101-cb9e-443c-aae0-8203654c262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The incorrect sentence is: {my_sentence.get_error_sentence()}')\n",
    "print(f'The correct sentence is: {my_sentence.get_correct_sentence()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b0481-e071-4f97-9526-7056e6682eb8",
   "metadata": {},
   "source": [
    "The `getErrorIndex` method can be used to return the index of the first error in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3b927-f910-4f9c-ba6a-3eac8e254262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The correct sentence is: {my_sentence.get_error_index()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e489521-78e1-4315-941e-f9038be4bb95",
   "metadata": {},
   "source": [
    "We can check if another sentence is a correction to our sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606e810-c895-43db-aa5e-c88f0c15ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(my_sentence.is_correction([\"i\", \"study\", \"at\", \"Deakin\", \"University\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509172be-bb00-4391-b90f-6d9899e207cd",
   "metadata": {},
   "source": [
    "You can use the `get` method to retrieve the `LexicalEntry` at a specified index and update it if needed using `update`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1edaf-22d4-4291-b14c-f9dc8f04d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print LexicalEntry at index 0\n",
    "print(my_sentence.get(0))\n",
    "# Update it with I and print sentence\n",
    "my_sentence.update(0, LexicalEntry('I'))\n",
    "print(my_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4ed92-d2fc-40e6-afa5-712004398003",
   "metadata": {},
   "source": [
    "Finally, let's clean out sentence from errors and return a clean sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998fa69-e8a1-4223-ae07-31a5f83dc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fixed_sentence = my_sentence.clean_sentence()\n",
    "print(my_fixed_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99276e5e-18e0-4986-ac2d-97799e400f98",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "The `Corpus` class represents a collection of sentences from a dataset and provides methods for processing and generating test cases with eligible spelling errors. Here's a breakdown of its key methods:\n",
    "\n",
    "- **Initialization:** The class is initialized with an optional filename pointing to the Holbrook dataset. If a filename is provided, the dataset is read and processed; otherwise, an empty corpus is created.\n",
    "- **Reading and Processing Data:** The `read_corpus` method reads data from a file, processes it into a list of `Sentence` objects, and populates the corpus.\n",
    "- **Processing Line:** The `processLine` method takes a line from the dataset, removes punctuation, converts to lowercase, and creates a `Sentence` object with `LexicalEntry` instances.\n",
    "- **Generating Test Cases:** The `generateTestCases` method creates a list of sentences with exactly one eligible spelling error by iterating through the corpus and selecting appropriate words for testing.\n",
    "- **Vocabulary Retrieval:** The `get_vocabulary` method retrieves the vocabulary from the corpus, consisting of unique words.\n",
    "\n",
    "In summary, the Corpus class serves as a container for sentences, offering methods for data initialization, processing, test case generation, and vocabulary retrieval. It plays a crucial role in facilitating the handling and manipulation of linguistic data within the context of spelling correction and language modeling.\n",
    "\n",
    "Let's first create our `Corpus` object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8c553-88b1-4df8-bb18-a3abe5902bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = Corpus('data/trainset.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e19f67-6f2e-4434-9162-50721ed0caad",
   "metadata": {},
   "source": [
    "We now print a few sentences from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768ffa2-7238-4e57-8cec-aa9e84410b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(my_corpus.corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f6d5f-6349-4349-9dfb-055fa229e14b",
   "metadata": {},
   "source": [
    "# The Noisy Channel Spell Correction Model\n",
    "\n",
    "The foundation of the noisy channel model lies in treating a misspelled word as if it underwent distortion while being transmitted through a noisy communication channel. The objective is to construct a model of this noisy channel and determine the true word by evaluating the likelihood of each candidate word given the observed misspelling. The Bayesian inference approach is employed, seeking the word $w$ that maximizes the conditional probability $P(w|x)$. Mathematically expressed as:\n",
    "\n",
    "$$\\hat{w} = \\underset{w\\in V}{\\operatorname{argmax}} P(w|x) $$\n",
    "\n",
    "\n",
    "this signifies choosing the word with the highest likelihood from the vocabulary $V$. Bayesian classification leverages Bayes' rule to transform this into $P(x|w)P(w) / P(x)$, and by dropping the denominator $P(x)$, we get  the following simplified formula:\n",
    "\n",
    "$$\\hat{w} = \\underset{w\\in V}{\\operatorname{argmax}}  \\overbrace{P(x|w)}^\\text{Channel Model} \\times  \\underbrace{P(w)}_\\text{Language Model} $$\n",
    "\n",
    "In essence, the model computes the most probable word given an observed misspelling by multiplying the prior $P(w)$ (**Language Model**) and the likelihood $P(x|w)$ (**the Channel Model**). The noisy channel algorithm, is then applied to correct non-word spelling errors by ranking candidate corrections according to the previous Equation and selecting the highest-ranked one. This process involves evaluating the likelihood (**Channel Model**) $P(x|w)$ and the prior (**Language Model**) $P(w)$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98cafa39-b3b3-44fe-8093-8dcdfde32ea8",
   "metadata": {},
   "source": [
    "## Channel Model\n",
    "\n",
    "The estimation of the likelihood  $P(x|w)$, referred to as the channel model or error model, is a crucial aspect of the noisy channel model. This model aims to capture the probability that a word  $w$ w will be mistyped, taking into account various factors. While a perfect model might consider factors like the typist's identity or handedness, a practical estimate can be obtained by examining the **Minimal Damerau-Levenshtein Edit Distance** between two strings, where edits are:\n",
    "\n",
    "- Insertion\n",
    "- Deletion\n",
    "- Substitution\n",
    "- Transposition of two adjacent letters\n",
    "\n",
    "For instance, letters like **'m'** and **'n'** are often substituted due to both their phonetic similarity and their adjacency on the keyboard. A simple model might estimate  $P(acress∣across)$ by analyzing the frequency of the letter **'e'** being substituted for **'o'** in a large corpus of errors. To compute these probabilities systematically, a confusion matrix is employed, which lists the number of times one element was confused with another. Specifically, the file `data/count_1edit.txt` which provides counts for all single-edit spelling correction edits. For example, the line `da|d 13` suggests that the correction of **'d'** to **'da'** has been observed 13 times in the data, whereas `e|i 917` indicates that the substitution of **'e'** to **'i'** has been observed 917 times in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c7b48-a121-49a2-93e3-daf290acb462",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"><b>Task 1</b></span>\n",
    "\n",
    "Let's build our Channel Model and by building the model to calculate our likelihood $P(x|w)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636f307-a591-4ee3-a1e1-7132b1379e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelModel:\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "    def __init__(self, edit_file, vocabulary):\n",
    "        \"\"\"\n",
    "        Initializes the ChannelModel with an optional edit file and a corpus.\n",
    "\n",
    "        Parameters:\n",
    "        - edit_file (str): Path to the edit file.\n",
    "        - vocabulary (set): The vocabulary used.\n",
    "        \"\"\"\n",
    "        self.edit_file = edit_file\n",
    "        self.edit_table = self.read_edit_table(self.edit_file)\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def compute_edit_probabilities(self, word):\n",
    "        \"\"\"\n",
    "        Computes p(x|word) edit model for all valid word x ** that are in the vocabulary**. \n",
    "        Returns a dictionary mapping x -> p(x|word).\n",
    "\n",
    "        Parameters:\n",
    "        - word (str): The input word.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary mapping possible corrections to their conditional probabilities p(x|word).\n",
    "        \"\"\"        \n",
    "        counts = {}\n",
    "\n",
    "        # Considering  deletions\n",
    "        counts.update(self.process_deletions(word))\n",
    "        # Considering insertions\n",
    "        counts.update(self.process_insertions(word))\n",
    "        # Considering transpositions\n",
    "        counts.update(self.process_transpositions(word))\n",
    "        # Considering replacements\n",
    "        counts.update(self.process_replacements(word))\n",
    "\n",
    "        # normalize counts. sum over them all, divide each entry by sum.\n",
    "        total = 0.0\n",
    "        for a, b in counts.items():\n",
    "            total += b\n",
    "        # We set the probability of no correction to .9. Hence, self count is set to 9*total\n",
    "        selfCount = max(9 * total, 1)\n",
    "        counts[word] = selfCount\n",
    "        total += selfCount\n",
    "        probs = {}\n",
    "\n",
    "        if total != 0.0:\n",
    "            for a, b in counts.items():\n",
    "                probs[a] = float(b) / total\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def process_deletions(self, word):\n",
    "        \"\"\"\n",
    "        This function processes deletions on a given word by checking if each resulting word, obtained by \n",
    "        deleting a single letter at a time, is present in the vocabulary. If a resulting word is found\n",
    "        in the vocabulary, it calculates the edit count and returns a dictionary containing these \n",
    "        resulting words along with their corresponding edit counts.\n",
    "\n",
    "        Parameters:\n",
    "        - word: The input word for which deletions need to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the resulting words after deletion and their corresponding edit counts.\n",
    "        \"\"\"        \n",
    "        counts = {}\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return counts\n",
    "\n",
    "    def process_insertions(self, word):\n",
    "        \"\"\"\n",
    "        This function processes insertions on a given word by checking if each resulting word, obtained by \n",
    "        inserting a single letter at a time, is present in the vocabulary. If a resulting word is found\n",
    "        in the vocabulary, it calculates the edit count and returns a dictionary containing these \n",
    "        resulting words along with their corresponding edit counts.\n",
    "\n",
    "        Parameters:\n",
    "        - word: The input word for which insertions need to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the resulting words after insertion and their corresponding edit counts.\n",
    "        \"\"\"   \n",
    "        counts = {}\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return counts\n",
    "\n",
    "    def process_transpositions(self, word):\n",
    "        \"\"\"\n",
    "        This function processes transpositions on a given word by checking if each resulting word, obtained by \n",
    "        transposing two letters at a time, is present in the vocabulary. If a resulting word is found\n",
    "        in the vocabulary, it calculates the edit count and returns a dictionary containing these \n",
    "        resulting words along with their corresponding edit counts.\n",
    "\n",
    "        Parameters:\n",
    "        - word: The input word for which transpositions need to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the resulting words after transposition and their corresponding edit counts.\n",
    "        \"\"\" \n",
    "        counts = {}\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return counts\n",
    "\n",
    "    def process_replacements(self, word):\n",
    "        \"\"\"\n",
    "        This function processes replacements on a given word by checking if each resulting word, obtained by \n",
    "        replacing a single letter at a time, is present in the vocabulary. If a resulting word is found\n",
    "        in the vocabulary, it calculates the edit count and returns a dictionary containing these \n",
    "        resulting words along with their corresponding edit counts.\n",
    "\n",
    "        Parameters:\n",
    "        - word: The input word for which replacements need to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the resulting words after replacement and their corresponding edit counts.\n",
    "        \"\"\" \n",
    "        counts = {}\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return counts\n",
    "\n",
    "    def read_edit_table(self, file_name):\n",
    "        \"\"\"\n",
    "        Reads in the string edit counts file and stores it in a dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - file_name (str): Path to the edit counts file.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing edit counts.\n",
    "        \"\"\"\n",
    "        edit_table = {}\n",
    "        with open(file_name, encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                contents = line.split(\"\\t\")\n",
    "                edit_table[contents[0]] = int(contents[1])\n",
    "\n",
    "        return edit_table\n",
    "\n",
    "    def edit_count(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Returns how many times substring s1 is edited/modified/corrected as s2.\n",
    "\n",
    "        Parameters:\n",
    "        - s1 (str): Original substring.\n",
    "        - s2 (str): Replacement substring.\n",
    "\n",
    "        Returns:\n",
    "        - int: The edit count.\n",
    "        \"\"\"\n",
    "        return self.edit_table.get(s1 + \"|\" + s2, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e54a26-63a0-4a68-a3ca-a40de552a2b4",
   "metadata": {},
   "source": [
    "Test you code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab882b-672b-49a8-91df-e92ce716ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Corpus('data/trainset.txt')\n",
    "model = ChannelModel(edit_file=\"data/count_1edit.txt\", vocabulary=c.get_vocabulary())\n",
    "\n",
    "word = 'read'\n",
    "assert model.process_deletions(word) == {'red': 285}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_insertions(word) == {'ready': 8}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_replacements(word) == {'dead': 15, 'head': 4, 'lead': 45, 'road': 295, 'real': 4}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "word = 'there'\n",
    "assert model.process_insertions(word) == {'theyre': 36, 'theres': 258}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_transpositions(word) == {'three': 189}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_replacements(word) == {'where': 3, 'these': 14}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "word = 'more'\n",
    "assert model.process_insertions(word) == {'moore': 24}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_replacements(word) == {'fore': 2, 'sore': 3, 'move': 2}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "word = 'hello'\n",
    "assert model.process_deletions(word) == {'hell': 18}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "word = 'dad'\n",
    "assert model.process_insertions(word) == {'dead': 85, 'dads': 15}, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert model.process_replacements(word) == {'bad': 37, 'had': 1, 'did': 559, 'day': 2}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "assert model.compute_edit_probabilities('read') == {'red': 0.04344512195121951, 'ready': 0.0012195121951219512, \n",
    "                                                    'dead': 0.0022865853658536584, 'head': 0.0006097560975609756, \n",
    "                                                    'lead': 0.006859756097560976, 'road': 0.04496951219512195, \n",
    "                                                    'real': 0.0006097560975609756, 'read': 0.9}, 'Test failed'\n",
    "print('Seccessful')\n",
    "\n",
    "assert model.compute_edit_probabilities('reda') == {'red': 0.1, 'reda': 0.9}, 'Test failed'\n",
    "print('Seccessful')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257c416-086a-48ed-8212-e68684a24607",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Language models are a fundamental component of natural language processing and artificial intelligence, playing a crucial role in understanding and generating human-like text. These models are designed to comprehend the intricate patterns, structures, and semantics inherent in language, enabling machines to interact with and generate coherent and contextually relevant text.\n",
    "\n",
    "At their core, language models are statistical and machine learning-based systems that learn the inherent rules and patterns of a language from vast amounts of textual data. Their primary objective is to predict the next word or sequence of words in a given context, harnessing the power of probabilistic relationships within a language. The ability to predict the likelihood of various word combinations empowers these models to capture syntactic, semantic, and contextual nuances, making them versatile tools for a wide array of applications.\n",
    "\n",
    "Here, your task is to implement three Language Models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022e31c-e266-4fb2-b911-b7c53b0127f0",
   "metadata": {},
   "source": [
    "### Uniform Language Model\n",
    "\n",
    "### <span style=\"color:red\"><b>Task 2</b></span>\n",
    "\n",
    "Here your task is to build a simple uniform language model, which is capable of training on a given corpus and calculating the log-probability of sentences based on a uniform distribution of words.\n",
    "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the uniform language model score is calculated as follows:\n",
    "$$\\text{Uniform LM Score}(s) = log(P(s))=\\sum_{w \\in s } log(\\frac{1}{|V|})$$\n",
    "where $|V|$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff59356-6ea9-45fc-b275-23f3a7144a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformLanguageModel:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Initialize your data structures in the constructor.\n",
    "    \n",
    "        Parameters:\n",
    "        - corpus: The corpus to train the language model.\n",
    "        \"\"\"\n",
    "        self.vocabulary = set()\n",
    "        self.train(corpus)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Takes a corpus and trains your language model.\n",
    "        Compute any counts or other corpus statistics in this function.\n",
    "  \n",
    "        Parameters:\n",
    "        - corpus: The corpus to train the language model.\n",
    "        \"\"\"\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "    \n",
    "    def score(self, sentence):\n",
    "        \"\"\"\n",
    "        Takes a list of strings as an argument and returns the log-probability of the\n",
    "        sentence using your language model. Use whatever data you computed in train() here.\n",
    "    \n",
    "        Parameters:\n",
    "        - sentence (list): A list of strings representing the input sentence.\n",
    "    \n",
    "        Returns:\n",
    "        - float: The log-probability of the sentence.\n",
    "        \"\"\"        \n",
    "        score = 0.0\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa193169-962a-4b4e-9a06-9e2d4bb9f2dc",
   "metadata": {},
   "source": [
    "Test your implementation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7b107-b67a-43ed-912f-9a353404a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
    "trainPath = 'data/trainset.txt'\n",
    "trainingCorpus = Corpus(trainPath)\n",
    "\n",
    "uniformLM = UniformLanguageModel(trainingCorpus)\n",
    "assert uniformLM.score(['I','am', 'Australian']) == -22.245525328839886, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert uniformLM.score(['I','love','Deakin','University']) == -29.66070043845318, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert uniformLM.score(['I','stud', 'CS','in','Australia']) == -37.07587554806648, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert uniformLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) == -66.73657598651965, 'Test failed'\n",
    "print('Seccessful')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521de28-3b4d-4c40-8ef1-512e19f7803d",
   "metadata": {},
   "source": [
    "### Laplace Unigram Language Model\n",
    "\n",
    "### <span style=\"color:red\"><b>Task 3</b></span>\n",
    "\n",
    "Your task here is to implement a unigram language model using Laplace (add-one) smoothing.  The score calculated should be computed by summing the logarithms of Laplace-smoothed counts for each word in the sentence and adjusting for the total word count.\n",
    "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the Laplace Unigram Language Model score is calculated as follows:\n",
    "$$\\text{Laplace Unigram LM Score}(s)=log(P(s))=\\sum_{w_i \\in s } log(P(w_i))$$\n",
    "\n",
    "where the **Laplace-smoothed unigram probability** $P(w_i)$ is calculated as:\n",
    "\n",
    "$$P(w_i)=\\frac{c(w_i)+1}{\\sum_{ w_j\\in V } c(w_j) +|V|}$$\n",
    "\n",
    "where $|V|$ is the size of the vocabulary and $c(w_i)$ is the count indicating the number of times $w_i$ appears in the training set.\n",
    "The Laplace smoothing involves adding 1 to both the count of $c(w_i)$  and the denominator (total count of all words plus the vocabulary size). This ensures that even words not observed in the training set have a non-zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c27dea-a254-40bf-af4a-5a269a693651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceUnigramLanguageModel:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"Initialize your data structures in the constructor.\"\"\"\n",
    "        # TODO your code here\n",
    "        self.total = 0\n",
    "        self.vocabulary = set()\n",
    "        self.LaplaceUnigramCounts = {}\n",
    "        self.train(corpus)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\" Takes a corpus and trains your language model. \n",
    "            Compute any counts or other corpus statistics in this function.\n",
    "        \"\"\"\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\" Takes a list of strings as argument and returns the log-probability of the \n",
    "            sentence using your language model. Use whatever data you computed in train() here.\n",
    "        \"\"\"       \n",
    "        score = 0.0\n",
    "        ## START YOU CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b8490-b67d-4ef7-acc0-d8cecead87e8",
   "metadata": {},
   "source": [
    "Test your implementation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e926ab-a281-4183-ac80-4b0f34b95c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
    "trainPath = 'data/trainset.txt'\n",
    "trainingCorpus = Corpus(trainPath)\n",
    "\n",
    "laplaceUnigramLM = LaplaceUnigramLanguageModel(trainingCorpus)\n",
    "assert laplaceUnigramLM.score(['I','am', 'Australian']) == -25.14565287682459, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceUnigramLM.score(['I','love','Deakin','University']) == -35.57756036152766, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceUnigramLM.score(['I','stud', 'CS','in','Australia']) == -42.56080392732966, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceUnigramLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) == -68.26327621084289, 'Test failed'\n",
    "print('Seccessful')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f762a69-aace-4593-9e57-60b966004f8a",
   "metadata": {},
   "source": [
    "### Laplace Bigram Language Model\n",
    "\n",
    "### <span style=\"color:red\"><b>Task 4</b></span>\n",
    "\n",
    "This LaplaceBigramLanguageModel class is designed to implement a bigram language model using Laplace (add-one) smoothing.  The train method processes the corpus, updating the bigram counts with Laplace smoothing. Similarly, the score method should calculate the log-probability of a given sentence based on the trained bigram language model, incorporating Laplace smoothing to handle unseen bigrams and unigrams. \n",
    "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the Laplace Bigram Language Model score is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Laplace Bigram Language Model}(s) &=log(\\prod_{w_i \\in s } P(w_i))\\\\\n",
    "&= log[P(w_1)P(w_2|w_1)P(w_3|w_2)\\cdots P(w_n|w_{n-1})]\\\\\n",
    "&= log(P(w_1))+log(P(w_2|w_1))+log(P(w_3|w_2))\\cdots +log(P(w_n|w_{n-1}))]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where the **Laplace-smoothed conditional probability** $P(w_i|w_{i-1})$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_i|w_{i-1})=\\frac{c(w_i,w_{i-1})+1}{ c(w_{i-1}) +|V|}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where $|V|$ is the size of the vocabulary, and $c(w_i)$  and $c(w_i,w_{i-1})$ are respectively unigram and bigrams in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256224c-ee50-4928-94fe-05ea6ec87986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceBigramLanguageModel:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Initialize your data structures in the constructor.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus: The corpus to train the language model.\n",
    "        \"\"\"\n",
    "        uniModel = LaplaceUnigramLanguageModel(corpus)\n",
    "        self.LaplaceUnigramCounts = uniModel.LaplaceUnigramCounts\n",
    "        self.LaplaceBigramCounts = {}\n",
    "        self.total = uniModel.total\n",
    "        self.train(corpus)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Takes a corpus and trains your language model.\n",
    "        Compute any counts or other corpus statistics in this function.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus: The corpus to train the language model.\n",
    "        \"\"\"\n",
    "        ## START YOUR CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "\n",
    "\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"\n",
    "        Takes a list of strings as an argument and returns the log-probability of the \n",
    "        sentence using your language model. Use whatever data you computed in train() here.\n",
    "\n",
    "        Parameters:\n",
    "        - sentence (list): A list of strings representing the input sentence.\n",
    "\n",
    "        Returns:\n",
    "        - float: The log-probability of the sentence.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        ## START YOUR CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce044c-bf5d-4b70-b31a-e6be6f8b0358",
   "metadata": {},
   "source": [
    "Test your implementation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db8ca8-22fa-49d8-bace-864c0a52344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
    "trainPath = 'data/trainset.txt'\n",
    "trainingCorpus = Corpus(trainPath)\n",
    "\n",
    "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
    "assert laplaceBigramLM.score(['I','am', 'Australian']) == -14.847658917530413, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceBigramLM.score(['I','love','Deakin','University']) == -22.252126012871237, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceBigramLM.score(['I','stud', 'CS','in','Australia']) == -29.747159786723298, 'Test failed'\n",
    "print('Seccessful')\n",
    "assert laplaceBigramLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) == -58.90693709044222, 'Test failed'\n",
    "print('Seccessful')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4812f1-7d38-4f2f-a7e5-e2c99a259c19",
   "metadata": {},
   "source": [
    "## Spell Correction Channel using Noisy Channel Model\n",
    "\n",
    "Now is time to consolidate all the components and assemble your SpellCorrect model, which comprises both the **Channel model** and the **Language Model**. The Channel model plays a pivotal role in estimating the likelihood of various corrections for observed misspelled words, while the Language Model contributes by evaluating the overall coherence and probability of entire sentences. By integrating these two crucial elements, your SpellCorrect model can intelligently correct spelling errors in a given text, making it a powerful tool for improving the accuracy and readability of textual content. \n",
    "\n",
    "### <span style=\"color:red\"><b>Task 5</b></span>\n",
    "\n",
    "Below, you will need to complete the `SpellCorrector` by generating the correct sentence. **The assumption made is that there is exactly one error in each sentence**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35658f4-203f-4e54-8faf-f2b7be695ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "    \"\"\"Holds edit model, language model, corpus. trains\"\"\"\n",
    "\n",
    "    def __init__(self, lm, vocabulary):\n",
    "        \"\"\"Initializes the language model and edit model.\n",
    "\n",
    "        Parameters:\n",
    "        - lm (LanguageModel): The language model.\n",
    "        - corpus: The corpus for training.\n",
    "        \"\"\"\n",
    "        self.language_model = lm\n",
    "        self.channel_model = ChannelModel('data/count_1edit.txt', vocabulary)\n",
    "\n",
    "    def evaluate(self, corpus):\n",
    "        \"\"\"Tests this speller on a corpus, returns a SpellingResult.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus: The corpus for evaluation.\n",
    "\n",
    "        Returns:\n",
    "        - SpellingResult: Result object containing correct and total counts.\n",
    "        \"\"\"\n",
    "        numCorrect = 0\n",
    "        numTotal = 0\n",
    "        test_data = corpus.generate_test_cases()\n",
    "        for sentence in test_data:\n",
    "            if sentence.is_empty():\n",
    "                continue\n",
    "            errorSentence = sentence.get_error_sentence()\n",
    "            hypothesis = self.get_likely_correct_sentence(errorSentence)\n",
    "            if sentence.is_correction(hypothesis):\n",
    "                numCorrect += 1\n",
    "            numTotal += 1\n",
    "        return SpellingResult(numCorrect, numTotal)\n",
    "\n",
    "    def get_likely_correct_sentence(self, sentence):\n",
    "        \"\"\"Takes a list of words, returns a corrected list of words.\n",
    "\n",
    "        Parameters:\n",
    "        - sentence (list): List of words to correct.\n",
    "\n",
    "        Returns:\n",
    "        - list: Corrected list of words.\n",
    "        \"\"\"\n",
    "        if len(sentence) == 0:\n",
    "            return []\n",
    "        argmax_i = 0\n",
    "        argmax_w = sentence[0]\n",
    "        maxscore = float('-inf')\n",
    "        maxlm = float('-inf')\n",
    "        maxedit = float('-inf')\n",
    "\n",
    "        ## START YOUR CODE HERE\n",
    "        pass\n",
    "        ## END\n",
    "        return argmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b24abc-f437-4e84-a665-845f7d985f2d",
   "metadata": {},
   "source": [
    "Test your implementation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064e5b5-6e4a-41f9-a5b3-cf6d700d051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
    "trainPath = 'data/trainset.txt'\n",
    "trainingCorpus = Corpus(trainPath)\n",
    "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
    "laplaceBigramSpell = SpellCorrector(laplaceBigramLM, trainingCorpus.get_vocabulary())\n",
    "\n",
    "assert laplaceBigramSpell.get_likely_correct_sentence(['I','lov','Australia']) == ['I', 'love', 'Australia'], 'Test failed'\n",
    "print('Seccessful')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95005b9c-fa16-48b5-901e-86f2d98cda7f",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "It's time to subject your SpellCorrector to a comprehensive evaluation, leveraging the diverse language models you have implemented. This evaluation marks a crucial phase in assessing the efficacy and performance of your SpellCorrector across various linguistic scenarios. By applying the distinct language models, including, uniform LM, unigram LM, and bigram LM, you can systematically analyze how well your SpellCorrector handles different types of language complexities, spelling errors, and contextual nuances. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f60fe-7cd7-4af5-88af-bef340094dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
    "trainPath = 'data/trainset.txt'\n",
    "trainingCorpus = Corpus(trainPath)\n",
    "\n",
    "devPath = 'data/devset.txt'\n",
    "devCorpus = Corpus(devPath)\n",
    "\n",
    "print('Uniform Language Model: ')\n",
    "uniformLM = UniformLanguageModel(trainingCorpus)\n",
    "uniformSpell = SpellCorrector(uniformLM, trainingCorpus.get_vocabulary())\n",
    "uniformOutcome = uniformSpell.evaluate(devCorpus)\n",
    "assert uniformOutcome.get_accuracy() == 0.06581740976645435, 'UniformLanguageModel accuracy is incorrect'\n",
    "print(str(uniformOutcome), '\\n')\n",
    "\n",
    "print('Laplace Unigram Language Model: ')\n",
    "laplaceUnigramLM = LaplaceUnigramLanguageModel(trainingCorpus)\n",
    "laplaceUnigramSpell = SpellCorrector(laplaceUnigramLM, trainingCorpus.get_vocabulary())\n",
    "laplaceUnigramOutcome = laplaceUnigramSpell.evaluate(devCorpus)\n",
    "assert laplaceUnigramOutcome.get_accuracy() == 0.11040339702760085, 'LaplaceUnigramLanguageModel accuracy is incorrect'\n",
    "print(str(laplaceUnigramOutcome), '\\n')\n",
    "\n",
    "\n",
    "print('Laplace Bigram Language Model: ')\n",
    "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
    "laplaceBigramSpell = SpellCorrector(laplaceBigramLM, trainingCorpus.get_vocabulary())\n",
    "laplaceBigramOutcome = laplaceBigramSpell.evaluate(devCorpus)\n",
    "assert laplaceBigramOutcome.get_accuracy() == 0.13588110403397027, 'LaplaceBigramLanguageModel accuracy is incorrect'\n",
    "print(str(laplaceBigramOutcome), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89f510-be60-4662-af3c-ae8b0bc506b6",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"><b>Task 7</b></span>\n",
    "\n",
    "To what extent did the outcomes of your constructed spelling correction system meet your initial expectations, and what valuable insights did you acquire from the system's performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45413194-7bb5-4022-83eb-6b0169789855",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "\n",
    "Congratulations on completing the assignment! Your dedication and effort are commendable. By successfully working through the coding exercises and written exercises, you have demonstrated a strong understanding Language Modeling and it's application to Spelling Correction.\n",
    "\n",
    "\n",
    "Congratulations on finishing this notebook! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Acknowledgement\n",
    "\n",
    "## About the Author\n",
    "\n",
    "This notebook was authored by Mohamed Reda Bouadjenek. He is a Senior Lecturer (Assistant Professor) of Applied Artificial Intelligence in the School of Information Technology at Deakin University, Australia.\n",
    "\n",
    "## Contact Information\n",
    "\n",
    "- **Email:** reda.bouadjenek@deakin.edu.au\n",
    "- **GitHub:** https://github.com/rbouadjenek/\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
